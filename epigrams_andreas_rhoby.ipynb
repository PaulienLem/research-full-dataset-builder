{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f96525f-33ff-46f8-8b0e-2ea0df791a47",
   "metadata": {},
   "source": [
    "# Epigrams Andreas Rhoby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7eed4-108a-4654-895f-7dc1491384d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from docx import Document\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"papluca/xlm-roberta-base-language-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='./.cache')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir='./.cache')\n",
    "label_mapping = {0: 'af', 1: 'ar', 2: 'bg', 3: 'bn', 4: 'de', 5: 'en', 6: 'es', 7: 'fr', 8: 'hi', 9: 'it',\n",
    "                 10: 'ja', 11: 'ko', 12: 'nl', 13: 'pl', 14: 'pt', 15: 'ru', 16: 'sw', 17: 'th', 18: 'tr', 19: 'ur',\n",
    "                 20: 'vi', 21: 'zh'}\n",
    "\n",
    "def detect_language(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    predictions = torch.softmax(outputs.logits, dim=1)\n",
    "    lang_id = torch.argmax(predictions, dim=1).item()\n",
    "    return label_mapping[lang_id]\n",
    "\n",
    "def is_german(text):\n",
    "    lang = detect_language(text)\n",
    "    return lang == \"de\"\n",
    "\n",
    "def extract_epigram_id(text):\n",
    "    match = re.match(r'^(Nr\\.\\s+[A-Za-z0-9\\-]+)\\)?', text)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def remove_line_nrs(text):\n",
    "    return re.sub(r'^\\d+\\t', '', text.strip())\n",
    "\n",
    "def is_greek(text):\n",
    "    greek_pattern = r'[\\u0370-\\u03FF\\u1F00-\\u1FFF]'\n",
    "    return bool(re.search(greek_pattern, text))\n",
    "\n",
    "def clean_document_title(filename):\n",
    "    filename = filename.replace('.docx', '')\n",
    "    filename = re.sub(r'^Rhoby_\\d+-', '', filename)\n",
    "    return filename\n",
    "\n",
    "def process_docx(file_path, work_title, doc_title):\n",
    "    doc = Document(file_path)\n",
    "    extracted_data = []\n",
    "    nr_value = None\n",
    "\n",
    "    for para in doc.paragraphs:\n",
    "        para_text = remove_line_nrs(para.text.strip())\n",
    "\n",
    "        if not para_text:\n",
    "            continue\n",
    "\n",
    "        new_nr_value = extract_epigram_id(para_text)\n",
    "        if new_nr_value:\n",
    "            nr_value = new_nr_value\n",
    "\n",
    "        if is_greek(para_text) or new_nr_value or nr_value:\n",
    "            extracted_data.append([para_text, nr_value, work_title, doc_title])\n",
    "\n",
    "    if not extracted_data:\n",
    "        return pd.DataFrame(columns=['Text', 'NR', 'work_title', 'doc_title'])\n",
    "\n",
    "    return pd.DataFrame(extracted_data, columns=['Text', 'NR', 'work_title', 'doc_title'])\n",
    "\n",
    "input_folder = \"rhoby\"\n",
    "output_folder = \"rhoby_processed\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for root, dirs, files in sorted(os.walk(input_folder), key=lambda x: x[0]):\n",
    "    for file in sorted(files):\n",
    "        if file.endswith(\".docx\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            work_title = os.path.basename(root)\n",
    "            doc_title = clean_document_title(file)\n",
    "            output_file = os.path.join(output_folder, f\"{work_title}_{doc_title}.csv\")\n",
    "\n",
    "            try:\n",
    "                df = process_docx(file_path, work_title, doc_title)\n",
    "\n",
    "                if df.empty:\n",
    "                    continue\n",
    "\n",
    "                if os.path.exists(output_file):\n",
    "                    continue\n",
    "\n",
    "                df = df[~df['Text'].apply(is_german)]\n",
    "\n",
    "                df_grouped = df.groupby(['NR', 'work_title', 'doc_title'], as_index=False).agg({'Text': lambda x: '\\n'.join(x)})\n",
    "\n",
    "                df_grouped['Text'] = df_grouped['Text'].str.split(\"——\").str[0]\n",
    "\n",
    "                df_grouped.to_csv(output_file, index=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "print(\"Processing complete. All processed files saved to rhoby_processed folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b01e0-f8bc-4924-b054-2bcab988ace0",
   "metadata": {},
   "source": [
    "Rename and add id info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e57e89-b138-4345-93aa-f889ae896be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/Downloads/final_epigrams_rhoby.csv')\n",
    "\n",
    "replacements = {\n",
    "    'A_steine_publikation': 'band3_stein',\n",
    "    'B_epigramme_iv_rhoby': 'ban4_illuminierten_handschriften',\n",
    "    'C_frmo_varia_habilfwf': 'band1_fresken_mosaiken',\n",
    "    'D_rhoby2_bandfertig': 'band2_ikonen_kleinkunst'\n",
    "}\n",
    "\n",
    "df['work_title'] = df['work_title'].replace(replacements)\n",
    "\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "def split_nr(nr):\n",
    "    if not isinstance(nr, str):\n",
    "        return None, None\n",
    "\n",
    "    match = re.search(r'Nr\\.\\s*([A-Za-z]*)(\\d+)', nr)\n",
    "    if match:\n",
    "        epigram_group = match.group(1) if match.group(1) else None\n",
    "        epigram_id = int(match.group(2))\n",
    "        return epigram_group, epigram_id\n",
    "    return None, None\n",
    "\n",
    "df[['epigram_group', 'epigram_id']] = df['NR'].apply(lambda x: pd.Series(split_nr(x)))\n",
    "\n",
    "df.to_csv('~/Downloads/final_epigrams_updated_volnames.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188986fe-4a63-4612-962b-02f844f63af5",
   "metadata": {},
   "source": [
    "Print missing nrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3604a9-cbc2-47b6-a5a4-d52d6c581cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "df = pd.read_csv('~/Downloads/final_epigrams_updated_volnames.csv')\n",
    "\n",
    "invalid_rows = df[df['epigram_id'].isna()]\n",
    "if not invalid_rows.empty:\n",
    "    print(\"\\nRows to be dropped due to invalid NR values:\")\n",
    "    display(invalid_rows)\n",
    "\n",
    "df = df.dropna(subset=['epigram_id'])\n",
    "\n",
    "df['epigram_id'] = df['epigram_id'].astype(int)\n",
    "\n",
    "df = df.sort_values(by=['work_title', 'doc_title'])\n",
    "\n",
    "missing_total = 0\n",
    "\n",
    "for (work_title, doc_title), group in df.groupby(['work_title', 'doc_title']):\n",
    "    epigram_ids = sorted(group['epigram_id'].tolist())\n",
    "\n",
    "    min_id, max_id = epigram_ids[0], epigram_ids[-1]\n",
    "\n",
    "    missing_numbers = sorted(set(range(min_id, max_id + 1)) - set(epigram_ids))\n",
    "\n",
    "    if missing_numbers:\n",
    "        print(f\"\\nWork Title: {work_title}\")\n",
    "        print(f\"Document Title: {doc_title}\")\n",
    "        print(f\"Epigram ID Range: {min_id}-{max_id}\")\n",
    "        print(f\"Numbers Missing: {', '.join(map(str, missing_numbers))}\\n\")\n",
    "        missing_total += len(missing_numbers)  # Increment the total count of missing numbers\n",
    "\n",
    "print(f\"\\nTotal Missing Numbers: {missing_total}\")\n",
    "print(f\"Dataframe Shape: {df.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-experiments",
   "language": "python",
   "name": "venv-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
